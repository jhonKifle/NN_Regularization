{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFlunLiJ0wikKzkZ5QN8Ky",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhonKifle/NN_Regularization/blob/main/NN_Regularized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MNIST classification using Regularization (L2)**\n",
        "\n",
        "In this demo, the previous classification is used to show for enhancing Overfitting of the model. Overfitting is a common problem in machine learning where a model learns the training data too well, capturing not only the underlying patterns but also noise and irrelevant details. As a result, the model performs well on the training data but fails to generalize to new, unseen data.\n",
        "\n",
        "L2 regularization helps prevent overfitting by adding a penalty term to the loss function that penalizes large weights.\n",
        "\n",
        "When L2 regularization is applied, the gradient descent algorithm updates the weights not only based on the original loss but also by including a term that shrinks the weights. The parameter update rule with L2 regularization becomes:\n",
        "\n",
        "The weight update for NN with no regularizations was as follows:\n",
        "\n",
        " $$\n",
        " w′=w−η\\frac{∂C_0}{∂w} \\tag{1}\n",
        " $$\n",
        " $$\n",
        "b'= b−η\\frac{∂C0}{∂b} \\tag{2}\n",
        "$$\n",
        "Introducing the regularizations (L2), the learning rule for the weights becomes:\n",
        " $$\n",
        " w′=w(1−\\frac{ηλ}{n})−η\\frac{∂C_0}{∂w} \\tag{3}\n",
        " $$\n",
        "Biases are typically not regularized, but weights are, as biases control the overall position of the function, while weights control its shape and complexity."
      ],
      "metadata": {
        "id": "EdvObS08Z8F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Y5Q6jG6J1zLB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3J2NbYsdZ6x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hot-encodding for output\n",
        "def one_hot(y):\n",
        "    one_hot_y = [np.zeros((10,1)) for a in y]\n",
        "    for i,e in zip(y,one_hot_y):\n",
        "      e[i] = 1\n",
        "    return one_hot_y\n",
        "#Dataset Preprocesing\n",
        "def preprocess():\n",
        "  #load dataset\n",
        "  train = pd.read_csv('/content/sample_data/mnist_train_small.csv')\n",
        "  test = pd.read_csv('/content/sample_data/mnist_test.csv')\n",
        "\n",
        "  #NB size of train, & test dataset in this case is 19999 and 9999 respectively.\n",
        "  #To make is fit to a size of batch of 10, I duplicate the first row as following 2 line of codes.\n",
        "  train = train._append(train.head(1))\n",
        "  test = test._append(test.head(1))\n",
        "\n",
        "  valid = train.sample(frac=0.2)\n",
        "  train = train.drop(valid.index)\n",
        "\n",
        "  train_X = train.iloc[:,1:]\n",
        "  train_y = train.iloc[:,0]\n",
        "\n",
        "  train_X = train_X/255\n",
        "  train_X = [np.reshape(x, (784, 1)) for x in train_X.values]\n",
        "\n",
        "  valid_X = valid.iloc[:,1:]\n",
        "  valid_X = valid_X/255\n",
        "  valid_X = [np.reshape(x, (784, 1)) for x in valid_X.values]\n",
        "\n",
        "  valid_y = valid.iloc[:,0]\n",
        "\n",
        "  test_X = test.iloc[:,1:]\n",
        "  test_X = test_X/255\n",
        "  test_X = [np.reshape(x, (784, 1)) for x in test_X.values]\n",
        "  test_y = test.iloc[:,0]\n",
        "\n",
        "  train_y = train_y.to_numpy()\n",
        "  train_y = one_hot(train_y)\n",
        "  valid_y = valid_y.to_numpy()\n",
        "  test_y = test_y.to_numpy()\n",
        "  return list(zip(train_X,train_y)),list(zip(valid_X,valid_y)),list(zip(test_X,test_y))"
      ],
      "metadata": {
        "id": "5zxmB7IJAbsE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AFiH7AmIhYK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "181361a5-b9dd-45e8-8f3b-272b95b31982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : 3588 / 4000 (89.7%)\n",
            "Epoch 1 : 3560 / 4000 (89.0%)\n",
            "Epoch 2 : 3665 / 4000 (91.625%)\n",
            "Epoch 3 : 3659 / 4000 (91.475%)\n",
            "Epoch 4 : 3658 / 4000 (91.45%)\n",
            "Epoch 5 : 3654 / 4000 (91.35%)\n",
            "Epoch 6 : 3660 / 4000 (91.5%)\n",
            "Epoch 7 : 3677 / 4000 (91.925%)\n",
            "Epoch 8 : 3674 / 4000 (91.85%)\n",
            "Epoch 9 : 3623 / 4000 (90.575%)\n",
            "Epoch 10 : 3669 / 4000 (91.725%)\n",
            "Epoch 11 : 3640 / 4000 (91.0%)\n",
            "Epoch 12 : 3682 / 4000 (92.05%)\n",
            "Epoch 13 : 3704 / 4000 (92.60000000000001%)\n",
            "Epoch 14 : 3618 / 4000 (90.45%)\n",
            "Epoch 15 : 3687 / 4000 (92.175%)\n",
            "Epoch 16 : 3642 / 4000 (91.05%)\n",
            "Epoch 17 : 3686 / 4000 (92.15%)\n",
            "Epoch 18 : 3667 / 4000 (91.675%)\n",
            "Epoch 19 : 3717 / 4000 (92.925%)\n",
            "Epoch 20 : 3717 / 4000 (92.925%)\n",
            "Epoch 21 : 3673 / 4000 (91.825%)\n",
            "Epoch 22 : 3698 / 4000 (92.45%)\n",
            "Epoch 23 : 3698 / 4000 (92.45%)\n",
            "Epoch 24 : 3670 / 4000 (91.75%)\n",
            "Epoch 25 : 3689 / 4000 (92.225%)\n",
            "Epoch 26 : 3723 / 4000 (93.075%)\n",
            "Epoch 27 : 3673 / 4000 (91.825%)\n",
            "Epoch 28 : 3678 / 4000 (91.95%)\n",
            "Epoch 29 : 3669 / 4000 (91.725%)\n"
          ]
        }
      ],
      "source": [
        "class Cost:\n",
        "  @staticmethod\n",
        "  def Entropy(a,y):\n",
        "    return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "  @staticmethod\n",
        "  def delta(a,y):\n",
        "    return (a-y)\n",
        "class NN:\n",
        "    def __init__(self, sizes,cost=Cost):\n",
        "        self.layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.cost = cost\n",
        "        self.weightInitializer()\n",
        "\n",
        "    # weights are inialized with gaussian variable of mean = 0 and sd = 1/sqrt(inputs)\n",
        "    def weightInitializer(self):\n",
        "        self.weights = [np.random.randn(y,x)   for x,y in zip(self.sizes[:-1],self.sizes[1:])]\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "    def feedforward(self,a):\n",
        "        for w,b in zip(self.weights,self.biases):\n",
        "            a = self.sigmoid(np.dot(w,a)+b)\n",
        "        return a\n",
        "    def sigmoid(self,z):\n",
        "        return 1.0/(1.0+np.exp(-z))\n",
        "    def sigmoid_prime(self,z):\n",
        "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
        "    def backprop(self,x,y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "          z = np.dot(w, activation)+b\n",
        "          zs.append(z)\n",
        "          activation = self.sigmoid(z)\n",
        "          activations.append(activation)\n",
        "\n",
        "\n",
        "        delta = (self.cost).delta(activations[-1],y) * self.sigmoid_prime(zs[-1])\n",
        "\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        for l in range(2,self.layers):\n",
        "            z = zs[-l]\n",
        "            sp = self.sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(),delta)*sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta,activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "    def update_mini_batch(self,mini_batch,eta,lmbda,n):\n",
        "      nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "      nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "      for x, y in mini_batch:\n",
        "        delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "      self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "      self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "    #Stochastic gradient Decsent with regularization (L2) - Lambda = 5.0\n",
        "    def SGD(self,training_data,epochs,mini_batch_size,eta,lmbda=5.0,test_data=None):\n",
        "      if test_data:\n",
        "        n_test = len(test_data)\n",
        "      for j in range(epochs):\n",
        "        random.shuffle(training_data)\n",
        "        for x in range(0,len(training_data),mini_batch_size):\n",
        "          self.update_mini_batch(training_data[x:x+mini_batch_size],eta,lmbda,len(training_data))\n",
        "        if test_data:\n",
        "          err = self.evaluate(test_data)\n",
        "          print(\"Epoch {} : {} / {} ({}%)\".format(j,err,n_test,100*(err/n_test)))\n",
        "        else:\n",
        "          print(\"Epoch {} complete\".format(j))\n",
        "    def evaluate(self,test_data):\n",
        "      test_results = [(np.argmax(self.feedforward(x)),y) for (x,y) in test_data]\n",
        "      return sum(int(x==y) for (x,y) in test_results)\n",
        "\n",
        "\n",
        "net = NN([784,30,10])\n",
        "training_set, valid_set, test_set = preprocess()\n",
        "net.SGD(training_set, 30, 10, 3.0, test_data=valid_set)\n",
        "\n"
      ]
    }
  ]
}